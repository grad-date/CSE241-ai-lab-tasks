{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN and Spelling Error Detection\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading essential packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, Input\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading misspelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mis-spelled.txt\", \"r\") as f:\n",
    "    file1 = f.readlines()\n",
    "# file1[0]\n",
    "\n",
    "# removing the newline characters and correctly spelled words\n",
    "misspelled = []\n",
    "for i in file1:\n",
    "    if i[0] != '$':\n",
    "        misspelled.append(i[:-1])\n",
    "# len(misspelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading correctly-spelled word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"correct-spelled.txt\",\"r\") as f:\n",
    "  file2 = f.readlines()\n",
    "# file2[0]\n",
    "\n",
    "# removing the newline characters\n",
    "correct = []\n",
    "for i in file2:\n",
    "  correct.append(i[:-1])\n",
    "# len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixing as [correctly-spelled : misspelled :: 10 : 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 10000\n",
    "q = 10*p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for i in range(p):\n",
    "  words.append([misspelled[i],0])\n",
    "for i in range(q):\n",
    "  words.append([correct[i],1])\n",
    "\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataset\n",
    "with open(\"words.txt\",\"w+\") as f:\n",
    "  for i in words:\n",
    "    f.write(i[0] + \",\" + str(i[1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = []\n",
    "marker = []\n",
    "\n",
    "for i in words:\n",
    "  word.append(i[0])\n",
    "  marker.append(i[1])\n",
    "    \n",
    "# print(X[6], y[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(word, marker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "symbol2idx = {}\n",
    "for k in x_train:\n",
    "  for x in k:\n",
    "    if symbol2idx.get(x.lower()) is None:\n",
    "      symbol2idx[x] = count\n",
    "      count += 1\n",
    "\n",
    "for k in x_test:\n",
    "  for x in k:\n",
    "    if symbol2idx.get(x.lower()) is None:\n",
    "      symbol2idx[x] = count\n",
    "      count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenized Set of Letters of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d': 1,\n",
       " 'a': 2,\n",
       " 't': 3,\n",
       " 'b': 4,\n",
       " 'o': 5,\n",
       " 'c': 6,\n",
       " 'e': 7,\n",
       " 'r': 8,\n",
       " 'u': 9,\n",
       " 'i': 10,\n",
       " 'n': 11,\n",
       " 'f': 12,\n",
       " 'm': 13,\n",
       " 'l': 14,\n",
       " 's': 15,\n",
       " 'p': 16,\n",
       " 'v': 17,\n",
       " 'h': 18,\n",
       " 'z': 19,\n",
       " 'y': 20,\n",
       " 'g': 21,\n",
       " 'k': 22,\n",
       " 'w': 23,\n",
       " 'q': 24,\n",
       " 'x': 25,\n",
       " 'j': 26,\n",
       " '_': 27,\n",
       " '-': 28,\n",
       " \"'\": 29,\n",
       " '.': 30}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the length of Largest Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq_train = []\n",
    "for k in x_train:\n",
    "  z = []\n",
    "  for x in k:\n",
    "    z.append(symbol2idx[x.lower()])\n",
    "  eq_train.append(z)\n",
    "\n",
    "len(eq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 1\n",
    "for x in eq_train:\n",
    "  if T < len(x):\n",
    "    T = len(x)\n",
    "    \n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27500"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq_test = []\n",
    "for k in x_test:\n",
    "  z = []\n",
    "  for x in k:\n",
    "    z.append(symbol2idx[x.lower()])\n",
    "  eq_test.append(z)\n",
    "\n",
    "len(eq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in eq_test:\n",
    "  if T < len(x):\n",
    "    T = len(x)\n",
    "    \n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making all the Lists Equal in Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eq_train)):\n",
    "  while(len(eq_train[i])<T):\n",
    "    eq_train[i].append(0)\n",
    "\n",
    "for i in range(len(eq_test)):\n",
    "  while(len(eq_test[i])<T):\n",
    "    eq_test[i].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(eq_train)\n",
    "x_test = np.array(eq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(T, ))\n",
    "x = Embedding(V, 20)(i)\n",
    "x = Conv1D(1024, 7, activation=\"relu\")(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(1024, 3, activation=\"relu\")(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(1024, 1, activation=\"relu\")(x)\n",
    "x = Conv1D(1024, 1, activation=\"relu\")(x)\n",
    "x = Conv1D(1024, 1, activation=\"relu\")(x)\n",
    "x = Conv1D(1024, 1, activation=\"relu\")(x)\n",
    "x = MaxPooling1D(1)(x)\n",
    "x = Dense(2048, activation=\"relu\")(x)\n",
    "x = Dense(2048, activation=\"relu\")(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(i, x)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2579/2579 [==============================] - 882s 342ms/step - loss: 0.3089 - accuracy: 0.9072 - val_loss: 0.2786 - val_accuracy: 0.9100\n",
      "Epoch 2/20\n",
      "2579/2579 [==============================] - 856s 332ms/step - loss: 0.2746 - accuracy: 0.9101 - val_loss: 0.2618 - val_accuracy: 0.9170\n",
      "Epoch 3/20\n",
      "2579/2579 [==============================] - 864s 335ms/step - loss: 0.2458 - accuracy: 0.9201 - val_loss: 0.2616 - val_accuracy: 0.9172\n",
      "Epoch 4/20\n",
      "2579/2579 [==============================] - 895s 347ms/step - loss: 0.2449 - accuracy: 0.9193 - val_loss: 0.2511 - val_accuracy: 0.9211\n",
      "Epoch 5/20\n",
      "2579/2579 [==============================] - 926s 359ms/step - loss: 0.2297 - accuracy: 0.9243 - val_loss: 0.2445 - val_accuracy: 0.9203\n",
      "Epoch 6/20\n",
      "2579/2579 [==============================] - 915s 355ms/step - loss: 0.2292 - accuracy: 0.9247 - val_loss: 0.2488 - val_accuracy: 0.9178\n",
      "Epoch 7/20\n",
      "2579/2579 [==============================] - 976s 378ms/step - loss: 0.2223 - accuracy: 0.9257 - val_loss: 0.2364 - val_accuracy: 0.9236\n",
      "Epoch 8/20\n",
      "2579/2579 [==============================] - 908s 352ms/step - loss: 0.2188 - accuracy: 0.9283 - val_loss: 0.2443 - val_accuracy: 0.9237\n",
      "Epoch 9/20\n",
      "2579/2579 [==============================] - 958s 371ms/step - loss: 0.2078 - accuracy: 0.9320 - val_loss: 0.2364 - val_accuracy: 0.9250\n",
      "Epoch 10/20\n",
      "2579/2579 [==============================] - 1000s 388ms/step - loss: 0.2038 - accuracy: 0.9332 - val_loss: 0.2386 - val_accuracy: 0.9229\n",
      "Epoch 11/20\n",
      "2579/2579 [==============================] - 930s 361ms/step - loss: 0.2044 - accuracy: 0.9325 - val_loss: 0.2371 - val_accuracy: 0.9235\n",
      "Epoch 12/20\n",
      "2579/2579 [==============================] - 962s 373ms/step - loss: 0.2067 - accuracy: 0.9326 - val_loss: 0.2521 - val_accuracy: 0.9229\n",
      "Epoch 13/20\n",
      "2579/2579 [==============================] - 920s 357ms/step - loss: 0.2003 - accuracy: 0.9327 - val_loss: 0.2465 - val_accuracy: 0.9237\n",
      "Epoch 14/20\n",
      "2579/2579 [==============================] - 857s 332ms/step - loss: 0.1939 - accuracy: 0.9359 - val_loss: 0.2635 - val_accuracy: 0.9242\n",
      "Epoch 15/20\n",
      "2579/2579 [==============================] - 965s 374ms/step - loss: 0.2005 - accuracy: 0.9339 - val_loss: 0.2504 - val_accuracy: 0.9248\n",
      "Epoch 16/20\n",
      "2579/2579 [==============================] - 909s 352ms/step - loss: 0.1957 - accuracy: 0.9345 - val_loss: 0.2432 - val_accuracy: 0.9229\n",
      "Epoch 17/20\n",
      "2579/2579 [==============================] - 928s 360ms/step - loss: 0.1930 - accuracy: 0.9362 - val_loss: 0.2472 - val_accuracy: 0.9223\n",
      "Epoch 18/20\n",
      "2579/2579 [==============================] - 891s 346ms/step - loss: 0.1911 - accuracy: 0.9362 - val_loss: 0.2624 - val_accuracy: 0.9238\n",
      "Epoch 19/20\n",
      "2579/2579 [==============================] - 867s 336ms/step - loss: 0.1963 - accuracy: 0.9358 - val_loss: 0.2526 - val_accuracy: 0.9186\n",
      "Epoch 20/20\n",
      "2579/2579 [==============================] - 900s 349ms/step - loss: 0.2039 - accuracy: 0.9318 - val_loss: 0.3734 - val_accuracy: 0.9235\n"
     ]
    }
   ],
   "source": [
    "my_model = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9085272550582886,\n",
       " 0.9126848578453064,\n",
       " 0.9178000092506409,\n",
       " 0.920127272605896,\n",
       " 0.921963632106781,\n",
       " 0.925151526927948,\n",
       " 0.9259818196296692,\n",
       " 0.9277818202972412,\n",
       " 0.9295212030410767,\n",
       " 0.9305575489997864,\n",
       " 0.9320605993270874,\n",
       " 0.9317636489868164,\n",
       " 0.9329212307929993,\n",
       " 0.933624267578125,\n",
       " 0.9334787726402283,\n",
       " 0.9335393905639648,\n",
       " 0.9356363415718079,\n",
       " 0.935903012752533,\n",
       " 0.9307818412780762,\n",
       " 0.9331818222999573]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy of trained words\n",
    "my_model.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9100000262260437,\n",
       " 0.9169636368751526,\n",
       " 0.9172363877296448,\n",
       " 0.9210545420646667,\n",
       " 0.9203454256057739,\n",
       " 0.9178181886672974,\n",
       " 0.9235818386077881,\n",
       " 0.9236727356910706,\n",
       " 0.9249818325042725,\n",
       " 0.9228727221488953,\n",
       " 0.9235454797744751,\n",
       " 0.9228727221488953,\n",
       " 0.92372727394104,\n",
       " 0.9241636395454407,\n",
       " 0.9248181581497192,\n",
       " 0.9228545427322388,\n",
       " 0.9223454594612122,\n",
       " 0.9237818121910095,\n",
       " 0.9185818433761597,\n",
       " 0.9234908819198608]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy of tested words\n",
    "my_model.history[\"val_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2969233989715576,\n",
       " 0.266635000705719,\n",
       " 0.2498001605272293,\n",
       " 0.24169543385505676,\n",
       " 0.2351873517036438,\n",
       " 0.22774413228034973,\n",
       " 0.2221507579088211,\n",
       " 0.21787495911121368,\n",
       " 0.2134140580892563,\n",
       " 0.20984987914562225,\n",
       " 0.205118790268898,\n",
       " 0.20651927590370178,\n",
       " 0.20015107095241547,\n",
       " 0.20086081326007843,\n",
       " 0.20068475604057312,\n",
       " 0.19932757318019867,\n",
       " 0.19362248480319977,\n",
       " 0.19373877346515656,\n",
       " 0.21317680180072784,\n",
       " 0.20116235315799713]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss of trained words\n",
    "my_model.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2785904109477997,\n",
       " 0.2617946267127991,\n",
       " 0.2615647315979004,\n",
       " 0.2510560154914856,\n",
       " 0.24453827738761902,\n",
       " 0.24883835017681122,\n",
       " 0.23639553785324097,\n",
       " 0.24426767230033875,\n",
       " 0.23638227581977844,\n",
       " 0.23861585557460785,\n",
       " 0.2371080070734024,\n",
       " 0.2520958185195923,\n",
       " 0.24652400612831116,\n",
       " 0.2634761333465576,\n",
       " 0.25040218234062195,\n",
       " 0.2432243824005127,\n",
       " 0.24720682203769684,\n",
       " 0.26243844628334045,\n",
       " 0.25262191891670227,\n",
       " 0.3734048306941986]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss of tested words\n",
    "my_model.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
